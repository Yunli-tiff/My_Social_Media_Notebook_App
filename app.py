import streamlit as st
import pandas as pd
import os
import time
import tempfile
import re
from utils.fetch_url import (
    fetch_page_html,
    extract_visible_text,
    extract_title,
    download_all_images,
    download_all_audio,
)
from utils.ocr import extract_text_from_image
from utils.whisper_asr import transcribe_audio
from utils.gpt import multilang_summarize_and_classify  # å¯ä»¥æ”¹æˆä½ è‡ªå·±çš„å¤šèªæ‘˜è¦å‡½å¼
from utils.search_filter import filter_notes
from utils.markdown_export import export_notes_to_md
from utils.notion_api import upload_to_notion
from utils.dropbox_export import upload_to_dropbox

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¼”åŠ©å‡½å¼ï¼šå¾æ–‡å­—ä¸­æ“·å–æ‰€æœ‰ URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
URL_REGEX = re.compile(
    r"https?://(?:www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}"
    r"\.[a-zA-Z0-9()]{1,6}\b(?:[-a-zA-Z0-9()@:%_\+.~#?&//=]*)"
)

def extract_urls_from_text(text: str) -> list[str]:
    """
    å¾ä¸€å¤§æ®µæ–‡å­—è£¡ï¼Œæ‰¾å‡ºæ‰€æœ‰ç¬¦åˆ URL_REGEX çš„é€£çµã€‚
    """
    return re.findall(URL_REGEX, text)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit åŸºæœ¬è¨­å®š
st.set_page_config(
    page_title="ç¤¾ç¾¤ç­†è¨˜ç‰†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ç¶²é æ¨™é¡Œ
st.title("ğŸ“Œ InstaNoteï¼šä½ çš„å°ˆå±¬AIç­†è¨˜ç‰†")
st.markdown(
    """
    é€™æ˜¯ä¸€å€‹ä¸»é¡Œå°å‘çš„AIäº’å‹•å¼ç­†è¨˜ç‰†ï¼Œèƒ½è®€å–ç¶²é æˆ–ä¸Šå‚³çš„å¤šç¨®å…§å®¹ï¼Œé€²è¡Œè‡ªå‹•åŒ–æ‘˜è¦ã€åˆ†é¡å’Œå„²å­˜ï¼Œ
    å¹«ä½ çœä¸‹å¤§é‡è³‡æ–™æ•´ç†çš„æ™‚é–“ï¼ç³»çµ±æ”¯æ´å¤šèªç³»æ¨¡å‹ï¼Œä»¥åŠç¯©é¸ã€åŒ¯å‡ºã€åŒæ­¥åˆ° Notion / Dropboxç­‰åŠŸèƒ½ï½
    ä½ åªéœ€è¦è²¼å…¥ä¸€å€‹æˆ–å¤šå€‹ç¶²å€ï¼Œæˆ–é å…ˆä¸Šå‚³ä¸‹è¼‰å¥½çš„åœ–ç‰‡ï¼éŸ³è¨Šï¼æ–‡å­—æª”ï¼Œ
    å°±å¯ä»¥ä¸€éµé«”é©—ä»¥ä¸ŠåŠŸèƒ½ã€‚InstaNoteçµ•å°æ˜¯æ‡¶äººã€Jäººçš„ç¦éŸ³ï¼
    """,
    unsafe_allow_html=True,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å´é‚Šæ¬„ï¼šæ“ä½œå€
with st.sidebar:
    st.header("ğŸ”§ æ“ä½œå€")

    # 1. ä¸Šå‚³æª”æ¡ˆï¼ˆåœ–ç‰‡/éŸ³è¨Š/æ–‡å­—ï¼‰
    st.subheader("ğŸ“¤ ä¸Šå‚³åœ–ç‰‡ / éŸ³è¨Š / ç´”æ–‡å­—æª”æ¡ˆ")
    upload_files = st.file_uploader(
        label="é»æ“Šä¸Šå‚³...", 
        type=["png", "jpg", "jpeg", "mp3", "wav", "txt"], 
        accept_multiple_files=True
    )

    # 2. è²¼å…¥ä¸€æˆ–å¤šå€‹ç¶²å€
    st.markdown("---")
    st.subheader("ğŸŒ è²¼å…¥ä¸€æˆ–å¤šå€‹ç¶²å€ (ä¸€è¡Œä¸€å€‹æˆ–ç©ºæ ¼åˆ†éš”)")
    paste_urls = st.text_area(
        label="è«‹ç›´æ¥è²¼ä¸Š URL (http:// æˆ– https:// é–‹é ­)",
        placeholder="https://example.com/page1  https://example.com/page2\næˆ–æ›è¡Œè²¼å¤šå€‹ç¶²å€...",
        height=100
    )
    process_urls_btn = st.button("â¡ï¸ è™•ç†è²¼å…¥çš„ç¶²å€", key="process_urls")

    # 3. é—œéµå­—æœå°‹
    st.markdown("---")
    keyword = st.text_input("ğŸ” é—œéµå­—æœå°‹", placeholder="æœå°‹ç­†è¨˜å…§å®¹...")

    # 4. ä¸»é¡Œåˆ†é¡ä¸‹æ‹‰ï¼šå‹•æ…‹ç”Ÿæˆï¼Œä¹‹å¾Œæœƒä¾ note_data å¡«å…¥
    category_options_placeholder = st.empty()

    # 5. Notion åŒæ­¥
    st.markdown("---")
    st.subheader("ğŸ“’ Notion åŒæ­¥")
    notion_token = st.text_input(
        "Notion Integration Token", 
        placeholder="è¼¸å…¥ä½ çš„ Notion API Token", 
        type="password"
    )
    notion_db_id = st.text_input(
        "Notion Database ID", 
        placeholder="è¼¸å…¥ç›®æ¨™ Database ID"
    )
    sync_notion_btn = st.button("â¡ï¸ åŒæ­¥åˆ° Notion", key="sync_notion")

    # 6. Dropbox åŒæ­¥
    st.markdown("---")
    st.subheader("ğŸ“ Dropbox åŒæ­¥")
    dropbox_token = st.text_input(
        "Dropbox Access Token", 
        placeholder="è¼¸å…¥ä½ çš„ Dropbox Token", 
        type="password"
    )
    sync_dropbox_btn = st.button("â¡ï¸ å‚™ä»½åˆ° Dropbox", key="sync_dropbox")

    # 7. Markdown ä¸‹è¼‰
    st.markdown("---")
    st.subheader("ğŸ“„ Markdown åŒ¯å‡º")
    export_md_btn = st.button("â¬‡ï¸ ä¸‹è¼‰ Markdown", key="export_md")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»è¦é‚è¼¯ï¼šnote_data åŒæ™‚å„²å­˜ã€Œä¸Šå‚³æª”æ¡ˆã€åŠã€Œè²¼å…¥ç¶²å€ã€çš„çµæœ
note_data = []

# â€”â€”â€”â€” 1) è™•ç†ï¼šä½¿ç”¨è€…ä¸Šå‚³æª”æ¡ˆ â€”â€”â€”â€”
if upload_files:
    st.sidebar.success(f"å·²ä¸Šå‚³ {len(upload_files)} ç­†æª”æ¡ˆï¼Œé–‹å§‹è™•ç†â‹¯â‹¯")
    with st.spinner("ğŸ“¦ é€²è¡Œ OCR/ASR + å¤šèªæ‘˜è¦åˆ†é¡â€¦"):
        for file in upload_files:
            # å¦‚æœä¸Šå‚³çš„æ˜¯ .txtï¼Œä¸”è£¡é¢å¯èƒ½æœ‰å¤šå€‹ URLï¼Œå…ˆæŠ½å‡ºä¾†æ‰¹æ¬¡è™•ç†
            if file.name.lower().endswith(".txt"):
                raw_text = file.read().decode("utf-8")
                urls = extract_urls_from_text(raw_text)

                if urls:
                    # è‹¥æŠ“åˆ°è‡³å°‘ä¸€å€‹ URLï¼Œå°±æŒ‰æ¯å€‹ URL è™•ç†
                    for url in urls:
                        try:
                            html = fetch_page_html(url)
                            title = extract_title(html)
                            text_content = extract_visible_text(html)

                            # å»ºç«‹è‡¨æ™‚è³‡æ–™å¤¾ï¼Œä¸‹è¼‰å¤šåª’é«”
                            tmp_dir = os.path.join(tempfile.gettempdir(), "url_fetch")
                            os.makedirs(tmp_dir, exist_ok=True)
                            imgs = download_all_images(html, url, tmp_dir)
                            audios = download_all_audio(html, url, tmp_dir)

                            # åœ–ç‰‡åš OCRã€éŸ³è¨Šåš ASR
                            for img_path in imgs:
                                text_content += "\n" + extract_text_from_image(open(img_path, "rb"))
                            for audio_path in audios:
                                text_content += "\n" + transcribe_audio(open(audio_path, "rb"))

                            summary, category, keywords = multilang_summarize_and_classify(text_content)

                            record = {
                                "type": "url_batch",
                                "source": url,
                                "url": url,
                                "title": title,
                                "content": text_content,
                                "summary": summary,
                                "category": category,
                                "keywords": keywords,
                                "media": imgs + audios
                            }
                            note_data.append(record)
                        except Exception as e:
                            st.sidebar.error(f"âŒ ç„¡æ³•è®€å–æˆ–è™•ç†ç¶²å€ï¼š{url}\nè«‹å…ˆä¸‹è¼‰å…§å®¹å†ä¸Šå‚³æª”æ¡ˆ ({e})")
                else:
                    # ç´”æ–‡å­—æª”ä½†ç„¡ URLï¼Œç›´æ¥ç•¶æˆä¸€ç­†ã€Œæ–‡å­—å…§å®¹ã€è·‘æ‘˜è¦
                    content = raw_text
                    summary, category, keywords = multilang_summarize_and_classify(content)
                    record = {
                        "type": "text",
                        "source": file.name,
                        "url": "",
                        "title": file.name,
                        "content": content,
                        "summary": summary,
                        "category": category,
                        "keywords": keywords,
                        "media": []
                    }
                    note_data.append(record)
                continue  # è™•ç†å®Œé€™å€‹ .txt æª”å¾Œï¼Œç¹¼çºŒä¸‹ä¸€å€‹ä¸Šå‚³æª”æ¡ˆ

            # é .txtï¼šåœ–ç‰‡ã€éŸ³è¨Šã€æˆ–å–®ç´”æ–‡å­—æª” (å…¶ä»–å‰¯æª”å)
            if file.type.startswith("image"):
                content = extract_text_from_image(file)
                media_paths = []
            elif file.type.startswith("audio"):
                content = transcribe_audio(file)
                media_paths = []
            else:
                content = file.read().decode("utf-8")
                media_paths = []

            summary, category, keywords = multilang_summarize_and_classify(content)
            record = {
                "type": "file",
                "source": file.name,
                "url": "",
                "title": file.name,
                "content": content,
                "summary": summary,
                "category": category,
                "keywords": keywords,
                "media": media_paths
            }
            note_data.append(record)

        time.sleep(0.5)
    st.sidebar.success("âœ… ä¸Šå‚³æª”æ¡ˆè™•ç†å®Œæˆï¼")

# â€”â€”â€”â€” 2) è™•ç†ï¼šä½¿ç”¨è€…ç›´æ¥è²¼å…¥ç¶²å€ â€”â€”â€”â€”
if process_urls_btn and paste_urls:
    # å¾ paste_urls å¤šè¡Œæ–‡å­—è£¡æŠ½å‡ºæ‰€æœ‰ URL
    urls = extract_urls_from_text(paste_urls)
    if not urls:
        st.sidebar.error("âŒ é€™æ®µæ–‡å­—ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ URLï¼Œè«‹ç¢ºèªæ ¼å¼æˆ–ç›´æ¥ä¸‹è¼‰å¾Œä¸Šå‚³æª”æ¡ˆã€‚")
    else:
        st.sidebar.success(f"å…±åµæ¸¬åˆ° {len(urls)} å€‹ç¶²å€ï¼Œé–‹å§‹æ‰¹æ¬¡æ“·å–â‹¯â‹¯")
        with st.spinner("ğŸŒ æ“·å–ä¸¦è™•ç†è²¼å…¥çš„ç¶²å€â€¦"):
            for url in urls:
                try:
                    html = fetch_page_html(url)
                    title = extract_title(html)
                    text_content = extract_visible_text(html)

                    tmp_dir = os.path.join(tempfile.gettempdir(), "url_fetch")
                    os.makedirs(tmp_dir, exist_ok=True)
                    imgs = download_all_images(html, url, tmp_dir)
                    audios = download_all_audio(html, url, tmp_dir)

                    for img_path in imgs:
                        text_content += "\n" + extract_text_from_image(open(img_path, "rb"))
                    for audio_path in audios:
                        text_content += "\n" + transcribe_audio(open(audio_path, "rb"))

                    summary, category, keywords = multilang_summarize_and_classify(text_content)

                    record = {
                        "type": "url",
                        "source": url,
                        "url": url,
                        "title": title,
                        "content": text_content,
                        "summary": summary,
                        "category": category,
                        "keywords": keywords,
                        "media": imgs + audios
                    }
                    note_data.append(record)
                except Exception as e:
                    st.sidebar.error(f"âŒ ç¶²å€ {url} è™•ç†å¤±æ•—ï¼š{e}\nè«‹å…ˆä¸‹è¼‰æª”æ¡ˆå†ä¸Šå‚³ã€‚")
            time.sleep(0.5)
        st.sidebar.success("âœ… è²¼å…¥ç¶²å€è™•ç†å®Œæˆï¼")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è‹¥ note_data æœ‰å…§å®¹ï¼Œå°‡å®ƒè½‰æˆ DataFrame ä¸¦é¡¯ç¤º
if note_data:
    notes_df = pd.DataFrame(note_data)

    # 1. æ›´æ–°ã€Œä¸»é¡Œåˆ†é¡ã€ä¸‹æ‹‰ï¼šå…ˆæ”¾ã€Œå…¨éƒ¨ã€å†ä¾åºæ”¾å¯¦éš› categories
    category_list = ["å…¨éƒ¨"] + sorted(notes_df["category"].unique().tolist())
    category = category_options_placeholder.selectbox(
        "ğŸ—‚ï¸ é¸æ“‡ä¸»é¡Œåˆ†é¡",
        category_list,
        index=0
    )

    # 2. ç¯©é¸ï¼šå…ˆæŒ‰é—œéµå­—ï¼Œå†æŒ‰ä¸»é¡Œåˆ†é¡
    filtered_df = filter_notes(
        notes_df.rename(columns={"category": "ä¸»é¡Œ", "content": "åŸæ–‡", "summary": "æ‘˜è¦"}),
        keyword=keyword,
        category=category
    )

    # 3. å·¦å´é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    col1, col2, col3 = st.columns(3)
    col1.metric("ğŸ”¢ ç¸½ç­†è¨˜æ•¸", len(notes_df))
    col2.metric("ğŸ“‘ ç¯©é¸å¾Œç­†è¨˜æ•¸", len(filtered_df))
    distinct_topics = filtered_df["ä¸»é¡Œ"].nunique()
    col3.metric("ğŸ“‚ ç¯©é¸å¾Œä¸»é¡Œæ•¸", distinct_topics)

    st.markdown("---")

    # 4. æŒ‰ã€Œä¸»é¡Œã€åˆ†çµ„ï¼Œå…©æ¬„é¡¯ç¤ºæ¯ç­†ç­†è¨˜
    grouped = filtered_df.groupby("ä¸»é¡Œ")
    for topic, group in grouped:
        st.subheader(f"ğŸ“‚ {topic} ({len(group)})")
        left_col, right_col = st.columns(2)
        for idx, row in group.iterrows():
            label = row["title"] or row["source"]
            with (left_col if (idx % 2 == 0) else right_col).expander(f"ğŸ“ {label}"):
                st.markdown(f"**æ‘˜è¦ï¼š** {row['æ‘˜è¦']}")
                st.markdown(f"**åŸæ–‡å…§å®¹ï¼š**\n{row['åŸæ–‡'][:1000]}{'...' if len(row['åŸæ–‡'])>1000 else ''}")

    # 5. æŒ‰éˆ•å›å‘¼ï¼šMarkdown åŒ¯å‡ºã€Notion åŒæ­¥ã€Dropbox
    if export_md_btn:
        export_path = export_notes_to_md(
            filtered_df.rename(columns={
                "category": "ä¸»é¡Œ",
                "content": "åŸæ–‡",
                "summary": "æ‘˜è¦",
                "keywords": "é—œéµå­—",
                "url": "ç¶²å€",
                "title": "æ¨™é¡Œ"
            }).to_dict("records"),
            path="notes_export.md"
        )
        with open(export_path, "rb") as f:
            st.sidebar.download_button(
                label="â¬‡ï¸ ä¸‹è¼‰ notes_export.md",
                data=f,
                file_name="notes_export.md",
                mime="text/markdown"
            )

    if sync_notion_btn:
        if not notion_token or not notion_db_id:
            st.sidebar.error("âš ï¸ è«‹å…ˆå¡«å¯« Notion Token èˆ‡ Database IDï¼")
        else:
            with st.spinner("ğŸ”„ åŒæ­¥åˆ° Notion ä¸­â€¦"):
                success_count = 0
                for _, row in filtered_df.iterrows():
                    try:
                        upload_to_notion(
                            page_id=notion_db_id,
                            summary=row["æ‘˜è¦"],
                            category=row["ä¸»é¡Œ"],
                            source_text=row["åŸæ–‡"],
                            notion_token=notion_token,
                            url=row["url"],
                            title=row["title"],
                            keywords=row["keywords"]
                        )
                        success_count += 1
                    except Exception as e:
                        st.sidebar.error(f"åŒæ­¥å¤±æ•—ï¼š{row['source']} ï¼ {e}")
                time.sleep(0.5)
            st.sidebar.success(f"âœ… å·²æˆåŠŸåŒæ­¥ {success_count} ç­†åˆ° Notionï¼")

    if sync_dropbox_btn:
        if not dropbox_token:
            st.sidebar.error("âš ï¸ è«‹å…ˆå¡«å¯« Dropbox Tokenï¼")
        else:
            tmp_md = export_notes_to_md(
                filtered_df.rename(columns={
                    "category": "ä¸»é¡Œ",
                    "content": "åŸæ–‡",
                    "summary": "æ‘˜è¦",
                    "keywords": "é—œéµå­—",
                    "url": "ç¶²å€",
                    "title": "æ¨™é¡Œ"
                }).to_dict("records"),
                path="notes_backup.md"
            )
            with st.spinner("â˜ï¸ å‚™ä»½åˆ° Dropbox ä¸­â€¦"):
                try:
                    dropbox_path = f"/notes_backup_{int(time.time())}.md"
                    upload_to_dropbox(
                        token=dropbox_token,
                        local_file_path=tmp_md,
                        dropbox_dest_path=dropbox_path
                    )
                    st.sidebar.success(f"âœ… å·²å‚™ä»½è‡³ Dropboxï¼š{dropbox_path}")
                except Exception as e:
                    st.sidebar.error(f"Dropbox å‚™ä»½å¤±æ•—ï¼š{e}")
                    raise
else:
    st.info("è«‹å…ˆåœ¨å·¦å´ã€Œæ“ä½œå€ã€ä¸Šå‚³æª”æ¡ˆã€æˆ–è²¼å…¥ç¶²å€ï¼Œç³»çµ±æ‰æœƒè‡ªå‹•ç”¢ç”Ÿç­†è¨˜ã€‚")
